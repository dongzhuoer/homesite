---
title: study note
author: Zhuoer Dong
date: '2019-08-06'
slug: study-note
categories: book
tags:
  - book-zhuoer
authors: []
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#classification-measurement"><span class="toc-section-number">1</span> classification measurement</a></li>
<li><a href="#optimizer"><span class="toc-section-number">2</span> Optimizer</a></li>
</ul>
</div>

<div id="classification-measurement" class="section level1" number="1">
<h1><span class="header-section-number">1</span> classification measurement</h1>
<p><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" class="uri">https://en.wikipedia.org/wiki/Sensitivity_and_specificity</a></p>
<p>把圈的半径不断扩大就成了 ROC 曲线，曲线的形状（AUC）由圆心位置来决定（圆心位于中央即基准线 y=x）</p>
<p>F1 score 用到了 recall 和 precision。因为 precision 只能把圆心向 True 移动来增大，而 recall 虽然也能移圆心，但还是增大半径最有效。这样就能保证找到最好的模型 。</p>
</div>
<div id="optimizer" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Optimizer</h1>
<ul>
<li><a href="https://ruder.io/optimizing-gradient-descent/" class="uri">https://ruder.io/optimizing-gradient-descent/</a></li>
</ul>
<p>RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numinator update rule. Adam, finally, adds bias-correction and momentum to RMSprop.</p>
</div>
